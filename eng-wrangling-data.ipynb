{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis: Educational Lab\n",
    "\n",
    "Welcome to the Data Analysis Lab! In this educational project, we aim to explore crucial concepts that form the foundation of data manipulation and analysis. This project is based on the <a href='https://www.coursera.org/learn/data-analysis-with-python'>Data Analysis with Python</a> course on the Coursera platform. The topic discussed here relates to the second week of the course and involves the practical application of concepts using a dataset collected from Kaggle: <a href='https://www.kaggle.com/datasets/shariful07/student-mental-health'>Student Mental health</a>.\n",
    "\n",
    "By the end of this project, we will have acquired essential skills to handle missing data, adjust data types as needed, standardize and normalize relevant attributes, visualize data through <a href='#4'>grouped bar charts using binning</a>, and convert categorical data into numerical indicator variables.\n",
    "\n",
    "We sincerely thank Coursera for providing not only a comprehensive and accessible educational platform but also for offering financial support (<a href='https://www.coursera.org/financial-aid'>financial aid</a>) that enables participation in this course. We express our deep gratitude to the community, whose sharing of knowledge and mutual support significantly enriches the learning experience for everyone.\n",
    "\n",
    "## <a href='#1'>Handling Missing Data</a>\n",
    "Robust data analysis requires the ability to effectively deal with missing data. There are various approaches to this challenge, from <a href='#1'>excluding incomplete entries</a> to <a href='#1'>imputing missing values</a>. We will explore these techniques, highlighting their applications and practical considerations.\n",
    "\n",
    "## <a href='#2'>Data Type Correction</a>\n",
    "Consistency in data types is crucial. In this lab, we will learn to <a href='#2'>identify and correct inappropriate data types</a>, ensuring that each variable is correctly interpreted. This is essential to avoid analysis errors and misinterpretation of data.\n",
    "\n",
    "## <a href='#3'>Standardization and Normalization</a>\n",
    "Standardizing and normalizing data are essential steps for <a href='#3'>comparing attributes on different scales</a>. We will understand the difference between these processes and when to apply each. In the end, we will have data ready for <a href='#3'>more accurate and meaningful analyses</a>.\n",
    "\n",
    "## <a href='#4'>Visualization with Grouped Bar Charts (Binning)</a>\n",
    "Visualization is a powerful tool in data analysis. We will cover the <a href='#4'>binning technique</a>, which groups data into intervals, allowing for a clearer and interpretable representation. Get ready to create <a href='#4'>grouped bar charts</a> that reveal patterns and trends in your datasets.\n",
    "\n",
    "## <a href='#5'>Categorical Data Conversion</a>\n",
    "Not all algorithms can directly handle categorical data. We will learn to <a href='#5'>convert these variables into numerical indicators</a>, making them compatible with a wide range of analytical techniques. This step is vital to ensure that all aspects of your dataset contribute to robust analyses.\n",
    "\n",
    "Throughout this project, we focus not only on performing tasks but also on understanding the reasons behind each step. Are we ready? Let's begin exploring and enhancing our data analysis skills!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Necessary Libraries\n",
    "\n",
    "Now, let's import the necessary libraries to start our project. Although a detailed explanation of this step is not included in this work, it is important to understand that importing libraries is a common practice in Python programming. To deepen this knowledge, we recommend exploring online resources such as the official Python documentation ([Python Documentation](https://docs.python.org/3/)) and specific tutorials available on learning platforms such as Coursera itself or sites like W3Schools. This skill will be valuable throughout the project and in future explorations in data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the CSV File\n",
    "\n",
    "Now, let's proceed with reading the CSV file stored locally. The path you see in the cell below corresponds to the file location on **my machine**. To continue and access the file on your machine, follow the steps below:\n",
    "\n",
    "1. **Download the file:**\n",
    "   - Visit the [Student Mental Health](https://www.kaggle.com/datasets/shariful07/student-mental-health) link on Kaggle.\n",
    "   - Download the available CSV file on the page.\n",
    "2. **Save the file on your machine:**\n",
    "   - After downloading, save the file in an accessible location on your machine.\n",
    "3. **Update the path in the code:**\n",
    "   - If you saved the file in a different directory, adjust the file path in the code to reflect the correct location.\n",
    "\n",
    "Alternatively, you can use the file I provided in the same project directory on GitHub. However, downloading the file from Kaggle is recommended as it provides additional information about the data that can be useful. This procedure is essential to ensure that the code works correctly and that you can explore the dataset effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Choose your gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>What is your course?</th>\n",
       "      <th>Your current year of Study</th>\n",
       "      <th>What is your CGPA?</th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Do you have Depression?</th>\n",
       "      <th>Do you have Anxiety?</th>\n",
       "      <th>Do you have Panic attack?</th>\n",
       "      <th>Did you seek any specialist for a treatment?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/7/2020 12:02</td>\n",
       "      <td>Female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>year 1</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/7/2020 12:04</td>\n",
       "      <td>Male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Islamic education</td>\n",
       "      <td>year 2</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/7/2020 12:05</td>\n",
       "      <td>Male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>BIT</td>\n",
       "      <td>Year 1</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/7/2020 12:06</td>\n",
       "      <td>Female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Laws</td>\n",
       "      <td>year 3</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/7/2020 12:13</td>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Mathemathics</td>\n",
       "      <td>year 4</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>13/07/2020 19:56:49</td>\n",
       "      <td>Female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>BCS</td>\n",
       "      <td>year 1</td>\n",
       "      <td>3.50 - 4.00</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>13/07/2020 21:21:42</td>\n",
       "      <td>Male</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Year 2</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>13/07/2020 21:22:56</td>\n",
       "      <td>Female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Year 3</td>\n",
       "      <td>3.50 - 4.00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>13/07/2020 21:23:57</td>\n",
       "      <td>Female</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pendidikan Islam</td>\n",
       "      <td>year 4</td>\n",
       "      <td>3.50 - 4.00</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>18/07/2020 20:16:21</td>\n",
       "      <td>Male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Biomedical science</td>\n",
       "      <td>Year 2</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Timestamp Choose your gender   Age What is your course?  \\\n",
       "0         8/7/2020 12:02             Female  18.0          Engineering   \n",
       "1         8/7/2020 12:04               Male  21.0    Islamic education   \n",
       "2         8/7/2020 12:05               Male  19.0                  BIT   \n",
       "3         8/7/2020 12:06             Female  22.0                 Laws   \n",
       "4         8/7/2020 12:13               Male  23.0         Mathemathics   \n",
       "..                   ...                ...   ...                  ...   \n",
       "96   13/07/2020 19:56:49             Female  21.0                  BCS   \n",
       "97   13/07/2020 21:21:42               Male  18.0          Engineering   \n",
       "98   13/07/2020 21:22:56             Female  19.0             Nursing    \n",
       "99   13/07/2020 21:23:57             Female  23.0     Pendidikan Islam   \n",
       "100  18/07/2020 20:16:21               Male  20.0   Biomedical science   \n",
       "\n",
       "    Your current year of Study What is your CGPA? Marital status  \\\n",
       "0                       year 1        3.00 - 3.49             No   \n",
       "1                       year 2        3.00 - 3.49             No   \n",
       "2                       Year 1        3.00 - 3.49             No   \n",
       "3                       year 3        3.00 - 3.49            Yes   \n",
       "4                       year 4        3.00 - 3.49             No   \n",
       "..                         ...                ...            ...   \n",
       "96                      year 1        3.50 - 4.00             No   \n",
       "97                      Year 2        3.00 - 3.49             No   \n",
       "98                      Year 3        3.50 - 4.00            Yes   \n",
       "99                      year 4        3.50 - 4.00             No   \n",
       "100                     Year 2        3.00 - 3.49             No   \n",
       "\n",
       "    Do you have Depression? Do you have Anxiety? Do you have Panic attack?  \\\n",
       "0                       Yes                   No                       Yes   \n",
       "1                        No                  Yes                        No   \n",
       "2                       Yes                  Yes                       Yes   \n",
       "3                       Yes                   No                        No   \n",
       "4                        No                   No                        No   \n",
       "..                      ...                  ...                       ...   \n",
       "96                       No                  Yes                        No   \n",
       "97                      Yes                  Yes                        No   \n",
       "98                      Yes                   No                       Yes   \n",
       "99                       No                   No                        No   \n",
       "100                      No                   No                        No   \n",
       "\n",
       "    Did you seek any specialist for a treatment?  \n",
       "0                                             No  \n",
       "1                                             No  \n",
       "2                                             No  \n",
       "3                                             No  \n",
       "4                                             No  \n",
       "..                                           ...  \n",
       "96                                            No  \n",
       "97                                            No  \n",
       "98                                            No  \n",
       "99                                            No  \n",
       "100                                           No  \n",
       "\n",
       "[101 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caminho do arquivo CSV localizado na máquina do autor (substitua pelo seu caminho)\n",
    "file = r'D:\\Jeanco\\Meus projetos\\students_mental_health\\Student Mental health.csv'\n",
    "\n",
    "# Leitura do arquivo CSV utilizando a biblioteca pandas\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# Visualização do DataFrame para verificar se o arquivo CSV foi lido corretamente\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization of Column Naming\n",
    "\n",
    "Before delving into the specific topics mentioned at the beginning of this project, we will choose to adopt more consistent naming conventions for the dataset columns. This practice aims to improve code readability and facilitate data manipulation, especially during programming processes. We will also use Brazilian Portuguese names for better readability in this context.\n",
    "\n",
    "## Adopted Conventions:\n",
    "\n",
    "1. **Lowercase and Underscores (Snake Case):**\n",
    "   - Example: `submission_time`, `choose_your_gender`, `age`, `course`, etc.\n",
    "\n",
    "## Why are we doing this?\n",
    "\n",
    "By adopting this approach, we make column names more programming-friendly, making it easier to reference and manipulate data. Additionally, this standardization contributes to consistency in code and analysis, enhancing clarity and understanding of the dataset.\n",
    "\n",
    "Let's apply these conventions as in the following example:\n",
    "\n",
    "```python\n",
    "# Example of column renaming\n",
    "df.rename(columns={\n",
    "    'Timestamp': 'timestamp',\n",
    "    'Choose your gender': 'gender',\n",
    "    'Age': 'age',\n",
    "    'What is your course?': 'course',\n",
    "    # ... (continue for other columns)\n",
    "}, inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>course</th>\n",
       "      <th>current_study_year</th>\n",
       "      <th>cgpa</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>have_depression</th>\n",
       "      <th>have_anxiety</th>\n",
       "      <th>have_panic_attack</th>\n",
       "      <th>seek_specialist_for_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/7/2020 12:02</td>\n",
       "      <td>Female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>year 1</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/7/2020 12:04</td>\n",
       "      <td>Male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Islamic education</td>\n",
       "      <td>year 2</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/7/2020 12:05</td>\n",
       "      <td>Male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>BIT</td>\n",
       "      <td>Year 1</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/7/2020 12:06</td>\n",
       "      <td>Female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Laws</td>\n",
       "      <td>year 3</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/7/2020 12:13</td>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Mathemathics</td>\n",
       "      <td>year 4</td>\n",
       "      <td>3.00 - 3.49</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp  gender   age             course current_study_year  \\\n",
       "0  8/7/2020 12:02  Female  18.0        Engineering             year 1   \n",
       "1  8/7/2020 12:04    Male  21.0  Islamic education             year 2   \n",
       "2  8/7/2020 12:05    Male  19.0                BIT             Year 1   \n",
       "3  8/7/2020 12:06  Female  22.0               Laws             year 3   \n",
       "4  8/7/2020 12:13    Male  23.0       Mathemathics             year 4   \n",
       "\n",
       "          cgpa marital_status have_depression have_anxiety have_panic_attack  \\\n",
       "0  3.00 - 3.49             No             Yes           No               Yes   \n",
       "1  3.00 - 3.49             No              No          Yes                No   \n",
       "2  3.00 - 3.49             No             Yes          Yes               Yes   \n",
       "3  3.00 - 3.49            Yes             Yes           No                No   \n",
       "4  3.00 - 3.49             No              No           No                No   \n",
       "\n",
       "  seek_specialist_for_treatment  \n",
       "0                            No  \n",
       "1                            No  \n",
       "2                            No  \n",
       "3                            No  \n",
       "4                            No  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Renomeando as colunas\n",
    "df.rename(columns={\n",
    "    'Timestamp': 'timestamp',\n",
    "    'Choose your gender': 'gender',\n",
    "    'Age': 'age',\n",
    "    'What is your course?': 'course',\n",
    "    'Your current year of Study': 'current_study_year',\n",
    "    'What is your CGPA?': 'cgpa',\n",
    "    'Marital status': 'marital_status',\n",
    "    'Do you have Depression?': 'have_depression',\n",
    "    'Do you have Anxiety?': 'have_anxiety',\n",
    "    'Do you have Panic attack?': 'have_panic_attack',\n",
    "    'Did you seek any specialist for a treatment?': 'seek_specialist_for_treatment'\n",
    "}, inplace=True)\n",
    "\n",
    "#Verificando as primeiras linhas da tabela\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"1\">1 - Handling Missing Data</a>\n",
    "\n",
    "**Motivation:**\n",
    "   - This analysis aims to understand the presence of missing data in the dataset, crucial for accurate analyses.\n",
    "   - In a more complex dataset, identifying and addressing missing data is essential for reliable analyses.\n",
    "\n",
    "**Implications:**\n",
    "   - Missing data can impact the quality and validity of analyses.\n",
    "   - Understanding the distribution of missing data is fundamental for choosing appropriate treatment strategies.\n",
    "\n",
    "This analysis is crucial to ensure that we can address missing data in an informed and effective manner in our dataset.\n",
    "\n",
    "Next, we will analyze the data for \"absences\" and proceed with the treatment, providing a practical explanation of the importance of dealing with these biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gender\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "age\n",
      "False    100\n",
      "True       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "course\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "current_study_year\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "cgpa\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "marital_status\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "have_depression\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "have_anxiety\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "have_panic_attack\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "seek_specialist_for_treatment\n",
      "False    101\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_ausentes = df.isnull()\n",
    "for coluna in dados_ausentes.columns.values.tolist():\n",
    "    print(dados_ausentes[coluna].value_counts())\n",
    "    print(\"\")\n",
    "df.columns.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's address the issue of missing data, considering that our dataset is simple for didactic purposes.\n",
    "\n",
    "Below is a step-by-step explanation of the code used above to handle missing data:\n",
    "\n",
    "1. **`missing_data = df.isnull()`:**\n",
    "   - We create a boolean DataFrame indicating True for missing values and False for present values in `df`.\n",
    "2. **`for column in missing_data.columns.values.tolist():`:**\n",
    "   - We iterate through each column in the missing data DataFrame.\n",
    "3. **`print(missing_data[column].value_counts())`:**\n",
    "   - We count and display the quantity of missing (True) and present (False) values in each column.\n",
    "4. **`print(df.columns.isnull())`:**\n",
    "   - We check if there are missing data in the labels of the columns in the original DataFrame. In other words, it indicates whether any column name is empty. This can be useful in situations where there is a need to check the integrity of column labels in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explain the outputs provided by our code. Let's take an example:\n",
    "\n",
    "```\n",
    "timestamp\n",
    "False    101\n",
    "Name: count, dtype: int64\n",
    "```\n",
    "\n",
    "Notice that we have the column name `timestamp`. The False represents the quantity of non-null data. In this case, we have 101 non-null values for the specified column.\n",
    "\n",
    "Now, let's examine the output for the ```age``` column:\n",
    "\n",
    "```\n",
    "age\n",
    "False    100\n",
    "True       1\n",
    "Name: count, dtype: int64\n",
    "```\n",
    "\n",
    "- **`False: 100`:** Indicates that there are 100 non-null values in the ```age``` column.\n",
    "- **`True: 1`:** Indicates that there is one null (missing) value in the ```age``` column.\n",
    "\n",
    "The other information present in the outputs is not relevant for understanding within the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the value at the location of the null value in the ```age``` column? Let's use the `unique()` method to obtain the unique values in the column and find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18. 21. 19. 22. 23. 20. 24. nan]\n"
     ]
    }
   ],
   "source": [
    "print(df['age'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above indicates that in the age column, we have values like 18 years, 21 years, and so on. Note the last value: nan.\n",
    "\n",
    "Now we've identified that our null value is represented by \"nan\" (not a number), allowing us to confidently proceed with handling this missing data. The consistent use of \"nan\" as the default representation brings benefits in terms of consistency and compatibility in numerical operations. It's not necessary to deeply understand these details; the key point is that using \"nan\" as the missing value representation is preferable to other representations.\n",
    "\n",
    "Understanding that we are dealing with \"nan\" allows us to employ specific strategies, such as substitution with the mean or most frequent value, effectively.\n",
    "\n",
    "**Importance of Knowing the Value in Null:**\n",
    "\n",
    "Knowing the specific value that occupies the place of null is crucial for making informed decisions on how to handle it. For example, if the null value were represented by a specific character, like \"?\", it would be necessary to identify and treat that character uniquely, avoiding ambiguities in numerical operations and ensuring coherence in the data.\n",
    "\n",
    "In summary, the choice of null value representation directly impacts how we deal with these values during data analysis, and opting for \"nan\" provides a standardized and efficient approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"#1.1\">1.1 - Replacement of Null Values with Mean</a>\n",
    "\n",
    "When dealing with attributes that have continuous data, the most appropriate approach to replacing missing values is by using the mean. In our dataset, the \"```age```\" (age) variable is continuous, representing values like 18, 21, 19, and so on. If some of these values are missing, we can fill them with the mean of the variable.\n",
    "\n",
    "Continuous data refers to variables that can take any value within a specific range, such as age, which can be represented by real numbers. In contrast to discrete data, which takes specific values and is often integers, continuous data can include fractions or decimal numbers.\n",
    "\n",
    "Using the code below, we replace the missing values in the  ```age``` column with the mean of that same column:\n",
    "```python\n",
    "# Example code for replacing missing values with mean in the ```age`` column\n",
    "mean_age = df['age'].mean()\n",
    "df['age'].fillna(mean_age, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.   21.   19.   22.   23.   20.   24.   20.53]\n"
     ]
    }
   ],
   "source": [
    "media_idade = df['age'].astype('float').mean(axis=0)\n",
    "df['age'].replace(np.nan, media_idade, inplace=True)\n",
    "print(df['age'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now we have the value 20.53 instead of 'nan'. This value is the mean of all the ages presented in the column.\n",
    "\n",
    "The strategy of replacing with the mean is adopted to maintain coherence in the distribution of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# <a href=\"#1.2\">1.2 - Replacement of Null Values with Most Frequent Value</a>\n",
    "\n",
    "### Exemplification\n",
    "\n",
    "When dealing with attributes that contain categorical data, the recommended approach for replacing missing values is to use the most frequent value in the category. Let's consider a scenario where we have a categorical column, for example, the ```marital status``` column, and some values are missing. The appropriate strategy would be to replace these values with the most frequent marital status.\n",
    "\n",
    "Categorical data refers to variables that represent discrete categories, such as \"Single,\" \"Married,\" or \"Divorced.\" When encountering missing values in this type of attribute, it is beneficial to fill them with the most common value in the category, maintaining the integrity of the distribution.\n",
    "\n",
    "Below is a generic code that exemplifies how to replace missing values with the most frequent value in the categorical column:\n",
    "\n",
    "```python\n",
    "most_frequent_marital_status = df['marital_status'].value_counts().idxmax()\n",
    "df['marital_status'].replace(np.nan, most_frequent_marital_status, inplace=True)\n",
    "```\n",
    "This approach ensures that missing values are filled in a representative manner, aiming to maintain cohesion in the distribution of categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 href=\"2\">2 - Data type correction</h2>\n",
    "\n",
    "Before we begin, we would like to emphasize that our data poses some additional challenges during conversion. This is intentional to showcase various possibilities and options of how data might appear in its raw form, i.e., how it comes from the source. Remember, this is not an exhaustive exploration but a comprehensive overview with applications of key concepts to help develop autonomy.\n",
    "\n",
    "**Motivation:**\n",
    "- Conversion is one of the steps in the data preparation process, ensuring that variables are in the correct format for subsequent analyses.\n",
    "- Choosing appropriate data types optimizes computational performance, facilitates mathematical operations, and contributes to a clearer interpretation of variables.\n",
    "\n",
    "**Implications:**\n",
    "- Failure to convert can result in inconsistencies in results, undermining the reliability of analyses.\n",
    "- Standardizing data types eases interpretation and execution of specific operations, enhancing the effectiveness of analyses.\n",
    "\n",
    "Next, we will explore the data type conversion in our dataset, providing practical examples and emphasizing the significance of this step in the manipulation and analysis process.\n",
    "\n",
    "Let's examine each column and determine the best approach to correct each data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                         object\n",
       "gender                            object\n",
       "age                              float64\n",
       "course                            object\n",
       "current_study_year                object\n",
       "cgpa                              object\n",
       "marital_status                    object\n",
       "have_depression                   object\n",
       "have_anxiety                      object\n",
       "have_panic_attack                 object\n",
       "seek_specialist_for_treatment     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see above are the data types in each column. On the right, we have the column names, and on the left, the type of data present. Our observation predominantly reveals two types: object and float64. In the context of Python, data can be represented in various ways, encompassing both numbers and texts. The object type is generally associated with textual data, while float64 represents numerical data.\n",
    "\n",
    "When considering how to correct the data, we need to think about how it will be used in the problem to be solved. For this task, we'll make some general considerations on how to handle this.\n",
    "\n",
    "Let's examine some columns and possibilities:\n",
    "\n",
    "**`timestamp`**: This column should be converted to the datetime data type since it represents the moment of submission.\n",
    "\n",
    "Columns with Yes and No values (`have_depression`, `have_anxiety`, `have_panic_attack`, `seek_specialist_for_treatment`):\n",
    "If these columns represent binary variables, i.e., values 0 and 1 or true and false, it is common practice to convert them to the boolean data type (True or False). This can simplify logical operations and future analyses. For our work, we will keep these columns as they are.\n",
    "\n",
    "**`current_study_year`**: It seems to be a categorical variable representing the current year of study. We can convert it to the categorical data type.\n",
    "\n",
    "**`cgpa`**: The term \"CGPA\" usually refers to \"Cumulative Grade Point Average\" in English or, in Portuguese, \"Média Cumulativa de Notas.\" CGPA is a metric commonly used in educational systems, especially in institutions that follow the grading system.\n",
    "This value is a weighted average of the grades obtained by students during their study period. It reflects the overall academic performance of the student over a certain period. Normally, grades are converted to a specific scale, and the average is calculated taking into account the credits or weights associated with each course.\n",
    "\n",
    "When conducting real-world analyses, remember to explore the dataset and its intrinsic characteristics thoroughly. For didactic reasons, we will focus on broader explanations in this educational context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Converting the \"momento_envio\" Column\n",
    "\n",
    "First, we'll discuss more complicated cases, like the one addressed below, but in general, the `astype()` method – which we'll talk about later – is a common way to convert the data type of a column. For a basic task like ours, this explanation is sufficient.\n",
    "\n",
    "In the `timestamp` column, the data includes date and time information. Whatever approach is adopted, it will need to handle different date formats, such as day/month/year, month/year/day, with or without hour information.\n",
    "\n",
    "For this project, we won't extensively correct the data in this column, as that would go beyond the simplified scope we aim to present here. However, we'll make some relevant and interesting considerations and provide, as an illustrative example, a conversion. Read carefully:\n",
    "\n",
    "When dealing with data that involves dates and times, we enter territory that can be filled with challenges and complexities. Handling this data is not as trivial as it may seem at first glance. Here are some reasons why data related to dates and times can pose considerable difficulties in its manipulation/transformation.\n",
    "\n",
    "**Various Formats**: Dates can be presented in various formats, such as \"day/month/year\" or \"year/month/day,\" and times may or may not include seconds. The variety of representations adds complexity to the conversion process.\n",
    "\n",
    "**Missing or Inconsistent Data**: Often, datasets may contain missing or inconsistently formatted dates. This can result in errors during the conversion to a standard format.\n",
    "\n",
    "**Time Zones**: In global environments, the presence of different time zones can further complicate the correct interpretation of dates and times, especially if this information is not clearly defined.\n",
    "\n",
    "**Presence of Additional Text**: Sometimes, dates and times can be \"mixed\" with other text data, requiring careful separation and conversion of this information.\n",
    "\n",
    "**Precision and Granularity**: Depending on the context, it may be necessary to work with different levels of temporal precision, such as days, hours, minutes, or even seconds. The precise manipulation of these levels of granularity adds layers of complexity.\n",
    "\n",
    "It is crucial to recognize that the process of transforming data like that present in the `timestamp` column can be significantly challenging and present different levels of complexity. In real-world situations, a comprehensive analysis of dates and times often involves detailed considerations and advanced methods.\n",
    "\n",
    "In our current work, we are focusing on more basic concepts to provide a solid understanding, but it is important to be aware that handling dates and times can become an extensive and specialized journey.\n",
    "\n",
    "Let's consider the unique values present in our column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['8/7/2020 12:02', '8/7/2020 12:04', '8/7/2020 12:05',\n",
       "       '8/7/2020 12:06', '8/7/2020 12:13', '8/7/2020 12:31',\n",
       "       '8/7/2020 12:32', '8/7/2020 12:33', '8/7/2020 12:35',\n",
       "       '8/7/2020 12:39', '8/7/2020 12:40', '8/7/2020 12:41',\n",
       "       '8/7/2020 12:43', '8/7/2020 12:46', '8/7/2020 12:52',\n",
       "       '8/7/2020 13:05', '8/7/2020 13:07', '8/7/2020 13:12',\n",
       "       '8/7/2020 13:13', '8/7/2020 13:15', '8/7/2020 13:17',\n",
       "       '8/7/2020 13:29', '8/7/2020 13:35', '8/7/2020 13:41',\n",
       "       '8/7/2020 13:58', '8/7/2020 14:05', '8/7/2020 14:27',\n",
       "       '8/7/2020 14:29', '8/7/2020 14:31', '8/7/2020 14:41',\n",
       "       '8/7/2020 14:43', '8/7/2020 14:45', '8/7/2020 14:47',\n",
       "       '8/7/2020 14:56', '8/7/2020 14:57', '8/7/2020 14:58',\n",
       "       '8/7/2020 15:07', '8/7/2020 15:08', '8/7/2020 15:09',\n",
       "       '8/7/2020 15:12', '8/7/2020 15:14', '8/7/2020 15:18',\n",
       "       '8/7/2020 15:27', '8/7/2020 15:37', '8/7/2020 15:47',\n",
       "       '8/7/2020 15:48', '8/7/2020 15:57', '8/7/2020 15:58',\n",
       "       '8/7/2020 16:08', '8/7/2020 16:21', '8/7/2020 16:22',\n",
       "       '8/7/2020 16:34', '8/7/2020 16:53', '8/7/2020 17:05',\n",
       "       '8/7/2020 17:37', '8/7/2020 17:46', '8/7/2020 17:50',\n",
       "       '8/7/2020 18:10', '8/7/2020 18:11', '8/7/2020 19:05',\n",
       "       '8/7/2020 19:32', '8/7/2020 20:36', '8/7/2020 21:21',\n",
       "       '8/7/2020 22:35', '9/7/2020 6:57', '9/7/2020 11:43',\n",
       "       '9/7/2020 11:57', '9/7/2020 13:15', '9/7/2020 18:24',\n",
       "       '13/07/2020 10:07:32', '13/07/2020 10:10:30',\n",
       "       '13/07/2020 10:11:26', '13/07/2020 10:12:18',\n",
       "       '13/07/2020 10:12:26', '13/07/2020 10:12:28',\n",
       "       '13/07/2020 10:14:46', '13/07/2020 10:33:47',\n",
       "       '13/07/2020 10:34:08', '13/07/2020 11:46:13',\n",
       "       '13/07/2020 11:49:02', '13/07/2020 11:54:58',\n",
       "       '13/07/2020 13:57:11', '13/07/2020 14:38:12',\n",
       "       '13/07/2020 14:48:05', '13/07/2020 16:15:13',\n",
       "       '13/07/2020 17:30:44', '13/07/2020 19:08:32',\n",
       "       '13/07/2020 19:56:49', '13/07/2020 21:21:42',\n",
       "       '13/07/2020 21:22:56', '13/07/2020 21:23:57',\n",
       "       '18/07/2020 20:16:21'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['timestamp'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing the unique values in the ```timestamp``` column, we observe some patterns and potential challenges that deserve attention:\n",
    "\n",
    "Format D/M/Y and H:M: Most values follow the \"day/month/year hour:minute\" pattern, as exemplified by '8/7/2020 12:02'. This is good news as it indicates considerable uniformity.\n",
    "\n",
    "Presence of Seconds: Some values, like '13/07/2020 10:07:32', include seconds information. This variation in format may require additional attention when performing conversion operations.\n",
    "\n",
    "Dates with Leading Zeros: Some dates have leading zeros in the day and month, such as '08/07/2020 18:11'. Although this does not affect conversion, it may be relevant to consider consistency in the data.\n",
    "\n",
    "### Other potential issues\n",
    "\n",
    "Variation in Separators: We could have a set of dates in which some values use slashes (\"/\") as separators while others use hyphens (\"-\"). This could be a potential source of complexity when standardizing the data.\n",
    "\n",
    "When dealing with these challenges, we must define an approach that takes into account the variety of formats without compromising the quality of the conversion. Thus, it will be possible to adjust the strategy to ensure effective and correct data manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other considerations about date and time data\n",
    "\n",
    "What happens if the list is extremely extensive? It becomes impractical and unproductive for the analyst to check value by value. When it comes to understanding the data structure in an extensive list, especially when dealing with it for the first time, analysts often adopt various approaches. Let's describe some possibilities without delving into each one in detail.\n",
    "\n",
    "**Visualization of Initial Values**: Analysts often start by visualizing the initial values in the list to identify patterns and initial formats. This can provide initial insights into the data structure.\n",
    "\n",
    "**Use of Descriptive Methods**: Applying descriptive methods, such as counting the frequency of different formats, can help identify predominant patterns and less common formats. Tools like the value_counts() function in pandas can be useful.\n",
    "\n",
    "**Random Sampling**: In extensive datasets, analysts may choose to randomly sample a portion of the data for a more detailed analysis. This helps to get an overview without examining each value individually.\n",
    "\n",
    "**Analysis of Differences and Variations**: When examining the list, analysts can look for variations in formats, such as the presence or absence of seconds, different separators, etc. This helps identify possible challenges in standardization.\n",
    "\n",
    "**Use of Computational Tools**: Computational tools and methods, such as regular expressions, can be employed to identify specific patterns in the data. These tools are effective for dealing with extensive lists.\n",
    "\n",
    "In the specific case of the \"momento_envio\" column, considering that we are working in a programming environment (Python), pandas functions, regular expressions, and string manipulation methods can be applied to explore and understand the different formats present in the list.\n",
    "\n",
    "Don't worry about all this; it's work that should be done day by day and can be developed, mainly with practice. So, consider this information as a mental trigger for when you start delving into the content and truly transforming columns like this, already having some prior knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, we present one of the possible approaches (simple for this case), which does not consider all the biases that could be encountered at the time of conversion (some biases were highlighted earlier).\n",
    "\n",
    "In the following code, when using the parameter format='%d/%m/%Y %H:%M:%S', we specify the expected format for the data in the 'timestamp' column. This is done to handle different formats and ensure the correct interpretation of date and time values. The presence of NaT values (Not a Time) indicates that there were non-convertible or missing values, and the parameter errors='coerce' is useful to handle these situations without generating errors. In other words, if any value could not be converted, it would then be transformed into 'NaT'.\n",
    "\n",
    "While this is an effective approach for the example, in practice, the variety of date and time formats may require a more in-depth analysis and perhaps even the use of regular expressions or more advanced methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatetimeArray>\n",
       "[                'NaT', '2020-07-13 10:07:32', '2020-07-13 10:10:30',\n",
       " '2020-07-13 10:11:26', '2020-07-13 10:12:18', '2020-07-13 10:12:26',\n",
       " '2020-07-13 10:12:28', '2020-07-13 10:14:46', '2020-07-13 10:33:47',\n",
       " '2020-07-13 10:34:08', '2020-07-13 11:46:13', '2020-07-13 11:49:02',\n",
       " '2020-07-13 11:54:58', '2020-07-13 13:57:11', '2020-07-13 14:38:12',\n",
       " '2020-07-13 14:48:05', '2020-07-13 16:15:13', '2020-07-13 17:30:44',\n",
       " '2020-07-13 19:08:32', '2020-07-13 19:56:49', '2020-07-13 21:21:42',\n",
       " '2020-07-13 21:22:56', '2020-07-13 21:23:57', '2020-07-18 20:16:21']\n",
       "Length: 24, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'momento_envio' column to datetime format, considering different formats\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce', format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Display the first rows of the DataFrame after the conversion\n",
    "df['timestamp'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Converting the \"current_sudy_year\" column\n",
    "\n",
    "Now we move on to conversions using the astype() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year 1', 'year 2', 'Year 1', 'year 3', 'year 4', 'Year 2', 'Year 3']\n",
       "Categories (7, object): ['Year 1', 'Year 2', 'Year 3', 'year 1', 'year 2', 'year 3', 'year 4']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['current_study_year'] = df['current_study_year'].astype('category')\n",
    "df['current_study_year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting the 'current_study_year' column to the categorical type and using the `unique()` method, we gained insight into the unique values present there. It is noticeable that there are variations in how the study years are represented, including different capitalizations (uppercase and lowercase).\n",
    "\n",
    "To standardize these values, we can choose to convert all values to lowercase, for example, and then reassess the unique values to ensure consistency in representation. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year 1', 'year 2', 'year 3', 'year 4']\n",
       "Categories (4, object): ['year 1', 'year 2', 'year 3', 'year 4']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converte the current_study_year column to categorical type\n",
    "df['current_study_year'] = df['current_study_year'].str.lower()\n",
    "\n",
    "# Standardize values by converting to lowercase\n",
    "df['current_study_year'] = df['current_study_year'].astype('category')\n",
    "\n",
    "# Display unique values after conversion\n",
    "df['current_study_year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the conversion to the categorical type and the subsequent standardization of values to lowercase, we achieved consistency in the representation of study years. The performed conversion ensures uniformity in the data, consequently facilitating future analyses and interpretations. Now we will proceed with the conversion of the `cgpa` column, which will bring us additional challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Converting the cgpa Column\n",
    "\n",
    "Before we delve into discussing the conversion of data in the `cgpa` column, let's take a look at the unique values in the column to gain an understanding of how they are presented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3.00 - 3.49', '3.50 - 4.00', '3.50 - 4.00 ', '2.50 - 2.99',\n",
       "       '2.00 - 2.49', '0 - 1.99'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cgpa'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 CGPA Column: Dealing with Interval Values\n",
    "\n",
    "While exploring the CGPA column, we noticed that the data is presented in a metric format, representing intervals such as '3.00 - 3.49', '3.50 - 4.00', etc. These intervals suggest specific ranges of Cumulative Grade Point Averages (CGPA), providing an overview of students' academic performance.\n",
    "\n",
    "Now, to make these intervals more numerically meaningful, we will turn to the use of regular expressions (regex). Let's understand how this works:\n",
    "\n",
    "#### Understanding Regular Expressions Simply:\n",
    "Regular expressions, or regex, are character patterns that make it easier to search and manipulate texts. In the context of the CGPA column, where values are in the format '3.00 - 3.49', the simple regular expression ```\\d+\\.\\d+``` helps us extract the numbers contained therein.\n",
    "\n",
    "- \\d: represents any digit from 0 to 9.\n",
    "- +: indicates that the preceding character (in our case, \\d) can appear one or more times.\n",
    "- \\.: searches for a decimal point, preceded by a backslash to be interpreted literally.\n",
    "\n",
    "Having said that, our next step is to convert these \"textual intervals\" into more representative numeric values. We understand that each interval covers a range of values, and to simplify the analysis, we will assume the average of this interval as its representative value. If you haven't grasped what this means yet, we will look for a value within the presented interval values, and this value will be the average between them.\n",
    "\n",
    "##### Choosing the Right Technique:\n",
    "As we've seen, the ```cgpa``` column has its data in textual (string) format, and we need to extract the numbers presented there. So, for extracting numbers from strings, using regular expressions is a common and powerful choice.\n",
    "\n",
    "##### How to Use:\n",
    "Understanding the pattern of the data we are trying to extract is helpful. In our case, the pattern was presented as we explained what regular expressions are. There are online resources and Python libraries, like re, that provide assistance in creating regular expressions. The intention is not to teach how to solve all possible cases, but to show the way data can be presented and how it impacts the choice of treatment in a given situation.\n",
    "\n",
    "Below, we've created a function called ```calculate_average```. This function uses the re library (regular expressions) to extract all decimal numbers (\\d+\\.\\d+) from a string representing an interval, such as '3.00 - 3.49'. The function then converts these numbers to float format and calculates the average between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the re library, which deals with regular expressions\n",
    "import re\n",
    "\n",
    "# Defining a function called calculate_mean\n",
    "def calculate_mean(interval):\n",
    "    # Using re.findall to find all decimal number patterns in the interval string\n",
    "    found_numbers = re.findall(r'\\d+\\.\\d+', interval)\n",
    "    \n",
    "    # Initializing an empty list to store the converted numbers\n",
    "    converted_numbers = []\n",
    "    \n",
    "    # Iterating over each found number in the list\n",
    "    for number in found_numbers:\n",
    "        # Converting each number to the float data type and adding it to the converted_numbers list\n",
    "        converted_numbers.append(float(number))\n",
    "    \n",
    "    # Calculating the mean of the numbers in the converted_numbers list\n",
    "    mean = sum(converted_numbers) / len(converted_numbers)\n",
    "    \n",
    "    # Returning the calculated mean\n",
    "    return mean\n",
    "\n",
    "# Applying the calculate_mean function to the 'cgpa' column of the DataFrame df\n",
    "df['cgpa'] = df['cgpa'].apply(calculate_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101 entries, 0 to 100\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   cgpa    101 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 940.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# Checking information about the 'cgpa' column\n",
    "df[['cgpa']].info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy used was designed in a didactic manner, aiming to facilitate understanding, especially for beginners. After applying this approach, we can observe that the \"cgpa\" column now displays the converted and properly handled values - check the DTYPE above in the line starting with a hashtag.\n",
    "\n",
    "Keep in mind that the presented approaches reflect some real-world nuances that could not be addressed using conversion with the `astype()` method alone. However, to conclude this section, we provide a link to the official Pandas documentation, where you can find detailed information about the `astype()` method and its use for data type conversions:\n",
    "\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\">Method: astype() -  Pandas Documentation</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
